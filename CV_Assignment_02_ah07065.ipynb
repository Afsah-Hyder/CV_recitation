{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoRCSI8COGSUDT3cBZFLjC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Afsah-Hyder/Computer-Vision-Course/blob/main/CV_Assignment_02_ah07065.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Computer Vision Assigment 02\n",
        "## Name: Afsah Hyder\n",
        "## ID: ah07065"
      ],
      "metadata": {
        "id": "B_9gb49t5kz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this question we will expand on your knowledge about Generative Adversarial Networks\n",
        "(GANs). Building on top of what we have already implemented, the primary task in this question\n",
        "is to implement a Deep Convolutional GAN (DCGAN).\n",
        "You can read about GANs from online resources and understand it’s working principles. To get\n",
        "an insight on the power of advanced GANs, check out this.\n",
        "Simply speaking, a DCGAN is a type of GAN that uses a CNN as discriminator whereas for the\n",
        "generator it uses an architecture similar to a CNN but instead of normal convolution layers, it is\n",
        "composed of transposed convolutions. You can read more about DCGAN in the original\n",
        "DCGAN paper.\n",
        "To successfully complete the question, you need to complete all the parts below. Make sure to\n",
        "cite all the sources used.\n",
        "1. Load any one of the datasets from the list below, that you think is interesting to\n",
        "work, on in your notebook (only the training dataset). We would recommend you\n",
        "guys to downscale the images i.e., 64x64 or 32x32 and if needed, also convert\n",
        "the image to greyscale. [5]\n",
        "List of Datasets:\n",
        "• Flowers\n",
        "• Aircrafts\n",
        "• Cute Dogs"
      ],
      "metadata": {
        "id": "FLXEzGQfh4SQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# Optional library for grayscale conversion\n",
        "from PIL import Image  # If you choose to convert to grayscale\n",
        "\n",
        "# Load the flowers dataset\n",
        "dataset, info = tfds.load(\n",
        "    'tf_flowers',\n",
        "    split='train',\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")\n",
        "\n",
        "# Print information about the dataset\n",
        "print(info)\n",
        "\n",
        "IMG_SIZE = 32\n",
        "\n",
        "def format_image(image, label):\n",
        "\n",
        "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "\n",
        "    # Normalisation\n",
        "    image = image/255.0\n",
        "    return image, label\n",
        "\n",
        "training_set = dataset.shuffle(300).map(format_image).prefetch(1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOrM9uIfi5t8",
        "outputId": "bcf31624-78f0-40c6-a45c-fdc1b9cf563f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tfds.core.DatasetInfo(\n",
            "    name='tf_flowers',\n",
            "    full_name='tf_flowers/3.0.1',\n",
            "    description=\"\"\"\n",
            "    A large set of images of flowers\n",
            "    \"\"\",\n",
            "    homepage='https://www.tensorflow.org/tutorials/load_data/images',\n",
            "    data_dir='/root/tensorflow_datasets/tf_flowers/3.0.1',\n",
            "    file_format=tfrecord,\n",
            "    download_size=218.21 MiB,\n",
            "    dataset_size=221.83 MiB,\n",
            "    features=FeaturesDict({\n",
            "        'image': Image(shape=(None, None, 3), dtype=uint8),\n",
            "        'label': ClassLabel(shape=(), dtype=int64, num_classes=5),\n",
            "    }),\n",
            "    supervised_keys=('image', 'label'),\n",
            "    disable_shuffling=False,\n",
            "    splits={\n",
            "        'train': <SplitInfo num_examples=3670, num_shards=2>,\n",
            "    },\n",
            "    citation=\"\"\"@ONLINE {tfflowers,\n",
            "    author = \"The TensorFlow Team\",\n",
            "    title = \"Flowers\",\n",
            "    month = \"jan\",\n",
            "    year = \"2019\",\n",
            "    url = \"http://download.tensorflow.org/example_images/flower_photos.tgz\" }\"\"\",\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a CNN based discriminator. Justify in no more than 2 lines why have you\n",
        "chosen your architecture for the discriminator. Remember to use only Pytorch in\n",
        "building your networks. [12+3]"
      ],
      "metadata": {
        "id": "K_YZqKyjjCcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets\n",
        "\n",
        "class CNNModel(nn.Module):\n",
        "  def __init__(self,size):\n",
        "    super(CNNModel, self).__init__()\n",
        "    self.conv_layers = nn.Sequential(\n",
        "        nn.Conv2d(3, 32, kernel_size=3, padding=1),  # Input channel = 3 (RGB)\n",
        "        nn.BatchNorm2d(32),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),      # output= 32*16*16\n",
        "\n",
        "        nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),     # output= 64*8*8\n",
        "\n",
        "        nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "        nn.BatchNorm2d(128),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2, stride=2),     # output= 128*4*4\n",
        "    )\n",
        "    self.fc_layers = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(128 * 4 * 4, 512),  # Calculate output size based on input dimensions\n",
        "        nn.BatchNorm1d(512),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.1),                 #not much data, so lower drop-out rates used\n",
        "\n",
        "        nn.Linear(512,145),\n",
        "        nn.BatchNorm1d(145),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.1),\n",
        "\n",
        "        nn.Linear(145, 5),\n",
        "        nn.Softmax(dim=1),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv_layers(x)\n",
        "    x = self.fc_layers(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "sdmd6QTZ50zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 02** In this question, you will have to read up a bit on the different types of GANS that are found:\n",
        "Style transfer GAN, CGANS, CycleGAN, SRGANS. Only reading the abstract and introduction\n",
        "of these papers will equip you to solve these questions, reading them fully is up to you. Then, go through the given situations and suggest the suitable GAN along with a short but\n",
        "suitable explanation:"
      ],
      "metadata": {
        "id": "CUT6EPZu6Bb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "a. A researcher gave a student the job to perform some preprocessing techniques on an image dataset. The student, while playing around with that data, applies a sufficiently large median blurring kernel to the images, but deletes the original files. These new images were padded correctly as to retain the original size of the images. Which GAN is best suited to revive the original HD images and why? [10]\n",
        "\n",
        "Super-Resolution Generative Adversarial Networks (SRGANs) are the most suitable architecture for restoring the missing detail in the blurred HD images. Unlike general-purpose GANs, SRGANs are specifically designed for super-resolution tasks. Their strength lies in a combined loss function that incorporates both an adversarial loss and a content loss. The adversarial loss trains the network to generate images that appear realistic to a discriminator network, mimicking natural high-resolution photos. The content loss, however, ensures the generated image retains the essential details and features present in the blurred version. This targeted approach makes SRGANs well-suited for recovering the lost information in the corrupted HD images."
      ],
      "metadata": {
        "id": "wQAwKUYY9LEM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "b. Interns at a computer vision company are tasked with adding different skin textures to\n",
        "images of pandas and regular bears. If presented with a picture of a regular bear, the\n",
        "skin should be changed to match that of a panda. They do not have paired images of\n",
        "pandas and bears that they can use as direct mappings of each other. They have a set\n",
        "of images of pandas, and a different set of images of bears. Which GAN is best suited to\n",
        "achieve this image-to-image translation task and why? [10]\n",
        "\n",
        "CycleGAN is ideal for this unpaired image-to-image translation task. Separate panda and bear image sets train two generators: one to create panda-like skin on bears and another for bear-like skin on pandas. The cycle consistency loss enforces that translating a bear to panda-like skin and back to \"bear\" should yield an image close to the original bear. This cyclical constraint ensures realistic skin texture swapping on new images, even without paired training data."
      ],
      "metadata": {
        "id": "pWmesD14FTv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "c. A daughter wants to give her Van Gogh-fanatic mother a present. So, she decides to\n",
        "turna set of family photos to a set that looks as if it's been painted by Van Gogh. Which\n",
        "GAN's best suited to achieve this task and how would she train her model (using what\n",
        "datasets, etc.)? [10]\n",
        "\n",
        "Style Transfer GAN can create the painted photos with a Van Gogh artistic style. It achieves this artistic mastery through two key mechanisms: \"style vectors\" that define the artistic essence, and a \"progressive training\" approach where the network gradually learns to generate high-resolution images. A technique called \"adaptive instance normalization\" empowers StyleGAN to adapt the learned style to various image content.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1LAykQo2OW4j"
      }
    }
  ]
}